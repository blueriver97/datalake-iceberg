{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690bbce70b566940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from attr import dataclass\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F  # noqa\n",
    "from pyspark.sql import types as T  # noqa\n",
    "\n",
    "# 1. 자격 증명 로드 (Boto3)\n",
    "source_credential = boto3.Session(profile_name=\"prod\").get_credentials()\n",
    "target_credential = boto3.Session(profile_name=\"qa\").get_credentials()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Catalog:\n",
    "    name: str\n",
    "    warehouse: str\n",
    "\n",
    "\n",
    "SOURCE = Catalog(name=\"source\", warehouse=\"s3a://hunet-di-data-lake-prod/iceberg\")\n",
    "TARGET = Catalog(name=\"target\", warehouse=\"s3a://hunet-di-data-lake-qa/iceberg\")\n",
    "\n",
    "# 2. Spark 세션 생성 (카탈로그 구조만 정의)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"IcebergMigrationFinal\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(f\"spark.sql.catalog.{SOURCE.name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{SOURCE.name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{SOURCE.name}.warehouse\", SOURCE.warehouse)\n",
    "    .config(f\"spark.sql.catalog.{SOURCE.name}.s3.path-style-access\", True)\n",
    "    .config(f\"spark.sql.catalog.{TARGET.name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{TARGET.name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{TARGET.name}.warehouse\", TARGET.warehouse)\n",
    "    .config(f\"spark.sql.catalog.{TARGET.name}.s3.path-style-access\", True)\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.sql.caseSensitive\", True)\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Java 시스템 프로퍼티 제어를 위한 JVM 게이트웨이\n",
    "jvm = spark.sparkContext._gateway.jvm\n",
    "\n",
    "\n",
    "def set_java_aws_credentials(cred):\n",
    "    \"\"\"Java SDK v2가 사용하는 시스템 프로퍼티를 강제로 덮어씀\"\"\"\n",
    "    jvm.java.lang.System.setProperty(\"aws.accessKeyId\", cred.access_key)\n",
    "    jvm.java.lang.System.setProperty(\"aws.secretAccessKey\", cred.secret_key)\n",
    "    if cred.token:\n",
    "        jvm.java.lang.System.setProperty(\"aws.sessionToken\", cred.token)\n",
    "    else:\n",
    "        jvm.java.lang.System.clearProperty(\"aws.sessionToken\")\n",
    "\n",
    "\n",
    "def migration(schema, table):\n",
    "    try:\n",
    "        # --- STEP 1: Account A 자격 증명 주입 및 데이터 읽기 ---\n",
    "        print(\"INFO: Accessing Account A (Prod)\")\n",
    "        set_java_aws_credentials(source_credential)\n",
    "\n",
    "        # 중요: collect()를 사용하여 데이터를 Python 메모리로 완전히 가져옵니다.\n",
    "        # Spark의 Lazy Evaluation 때문에 나중에 쓰려고 하면 인증 정보가 꼬일 수 있습니다.\n",
    "        source_df = spark.read.table(f\"{SOURCE.name}.{schema}.{table}\")\n",
    "\n",
    "        print(\"INFO: Materializing data from Account A...\")\n",
    "        # 데이터가 너무 크면 .collect() 대신 temporary path에 저장을 고려해야 합니다.\n",
    "        # rows = source_df.collect()\n",
    "        # local_df = spark.createDataFrame(rows, schema=source_df.schema)\n",
    "\n",
    "        source_df.write.mode(\"overwrite\").format(\"parquet\").save(\"/Users/kimyj/tmp_spark\")\n",
    "        local_df = spark.read.format(\"parquet\").load(\"/Users/kimyj/tmp_spark\")\n",
    "\n",
    "        # --- STEP 2: Account B 자격 증명 주입 및 데이터 쓰기 ---\n",
    "        print(\"INFO: Switching to Account B (QA)\")\n",
    "        set_java_aws_credentials(target_credential)\n",
    "\n",
    "        # 이제 환경 변수가 아닌 Java System Property가 B 계정이므로\n",
    "        # Glue Catalog B와 S3 B에 정상적으로 접근합니다.\n",
    "        # spark.sql(f\"CREATE DATABASE IF NOT EXISTS {TARGET.name}.{schema}\")\n",
    "        (local_df.writeTo(f\"{TARGET.name}.{schema}.{table}\").tableProperty(\"format-version\", \"2\").createOrReplace())\n",
    "        print(\"INFO: Data migration completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Migration failed during JVM execution: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "print(\"INFO: Accessing Account A (Prod)\")\n",
    "set_java_aws_credentials(source_credential)\n",
    "spark.catalog.setCurrentCatalog(SOURCE.name)\n",
    "tables = spark.catalog.listTables(\"\")\n",
    "for i, table in enumerate(tables, start=1):\n",
    "    print(f\"{table.namespace[0]}.{table.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cd7d016e345e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "\n",
    "try:\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        schema, table = table.split(\".\")\n",
    "        print(f\"INFO: Migrating table {i}...{schema}.{table}\")\n",
    "        migration(schema=schema, table=table)\n",
    "finally:\n",
    "    # 작업 종료 후 보안을 위해 시스템 프로퍼티 초기화\n",
    "    jvm.java.lang.System.clearProperty(\"aws.accessKeyId\")\n",
    "    jvm.java.lang.System.clearProperty(\"aws.secretAccessKey\")\n",
    "    jvm.java.lang.System.clearProperty(\"aws.sessionToken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
