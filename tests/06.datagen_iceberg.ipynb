{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = \"blueriver\"\n",
    "\n",
    "CATALOG = \"glue_catalog\"\n",
    "ICEBERG_S3_ROOT_PATH = \"s3a://blueriver-datalake/iceberg\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"1\")\n",
    "    .config(\"spark.sql.defaultCatalog\", CATALOG)\n",
    "    .config(f\"spark.sql.catalog.{CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{CATALOG}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{CATALOG}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .config(f\"spark.sql.catalog.{CATALOG}.warehouse\", ICEBERG_S3_ROOT_PATH)\n",
    "    .config(f\"spark.sql.catalog.{CATALOG}.s3.path-style-access\", True)\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"spark.sql.caseSensitive\", True)\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c69bfab8677e9c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO. 데이터 생성 수 및 비율\n",
    "TRANSACTIONS_PER_SECOND = 10  # 초당 트랜잭션 수\n",
    "TOTAL_DURATION_SECONDS = 1  # 총 실행 시간 (초)\n",
    "INSERT_RATIO = 0.60  # 삽입 비율\n",
    "UPDATE_RATIO = 0.20  # 갱신 비율\n",
    "DELETE_RATIO = 0.20  # 삭제 비율\n",
    "\n",
    "# TODO. 테이블 목록 (<schema>.<table>)\n",
    "TABLE_LIST = \"\"\"store_bronze.tb_lower\"\"\".split(\",\")\n",
    "\n",
    "# Faker 생성\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "# Iceberg 테이블에서 스키마 가져오기 함수\n",
    "def get_table_schema(table_name: str):\n",
    "    df = spark.read.format(\"iceberg\").load(f\"{table_name}\")\n",
    "    schema = df.schema\n",
    "    return schema\n",
    "\n",
    "\n",
    "# 데이터 생성 함수\n",
    "def generate_data(schema):\n",
    "    data = {}\n",
    "    for field in schema.fields:\n",
    "        type_name = field.dataType.simpleString()\n",
    "\n",
    "        # 10% 확률로 NULL 값 생성\n",
    "        if field.nullable and random.random() < 0.1:\n",
    "            data[field.name] = None\n",
    "            continue\n",
    "\n",
    "        # 데이터 타입에 따른 값 생성\n",
    "        if \"int\" in type_name:\n",
    "            data[field.name] = fake.random_int(min=0, max=2147483647)  # 32-bit 정수\n",
    "        elif \"bigint\" in type_name:\n",
    "            data[field.name] = fake.random_int(min=0, max=9223372036854775807)  # 64-bit 정수\n",
    "        elif \"string\" in type_name:\n",
    "            max_length = 255  # 문자열 길이 제한\n",
    "            data[field.name] = fake.text(max_nb_chars=max_length).strip()\n",
    "        elif \"double\" in type_name:\n",
    "            data[field.name] = fake.pyfloat(left_digits=6, right_digits=3, positive=True)\n",
    "        elif \"decimal\" in type_name:\n",
    "            # decimal(precision, scale) 처리\n",
    "            precision, scale = map(int, type_name.replace(\"decimal(\", \"\").replace(\")\", \"\").split(\",\"))\n",
    "            data[field.name] = fake.pydecimal(left_digits=precision - scale, right_digits=scale, positive=True)\n",
    "        elif \"boolean\" in type_name:\n",
    "            data[field.name] = fake.boolean()\n",
    "        elif \"binary\" in type_name:\n",
    "            max_length = 65535  # 기본 바이너리 크기 (64 KB)\n",
    "            data[field.name] = fake.binary(length=random.randint(1, max_length))\n",
    "        elif \"date\" in type_name:\n",
    "            data[field.name] = fake.date_this_decade()\n",
    "        elif \"timestamp\" in type_name:\n",
    "            data[field.name] = fake.date_time_this_decade()\n",
    "        else:\n",
    "            # 알 수 없는 타입일 경우 기본값 처리\n",
    "            data[field.name] = None\n",
    "\n",
    "    data[\"last_applied_date\"] = datetime.datetime.now(datetime.UTC)\n",
    "    return data\n",
    "\n",
    "\n",
    "# 데이터 삽입 함수\n",
    "def insert_data(table_name, schema, num_records):\n",
    "    records = [generate_data(schema) for _ in range(num_records)]\n",
    "    df = spark.createDataFrame(records, schema=schema)\n",
    "    df.write.format(\"iceberg\").mode(\"append\").save(f\"{table_name}\")\n",
    "\n",
    "\n",
    "# 데이터 갱신 함수\n",
    "def update_data(table_name, schema, num_records):\n",
    "    df = spark.read.format(\"iceberg\").load(f\"{table_name}\")\n",
    "    updated_data = []\n",
    "    for row in df.take(num_records):\n",
    "        record = generate_data(schema)\n",
    "        record[\"id_iceberg\"] = row.id_iceberg\n",
    "        updated_data.append(record)\n",
    "    updated_df = spark.createDataFrame(updated_data, schema=schema)\n",
    "    updated_df.write.format(\"iceberg\").mode(\"overwrite\").save(f\"{table_name}\")\n",
    "\n",
    "\n",
    "# 데이터 삭제 함수\n",
    "def delete_data(table_name, num_records):\n",
    "    df = spark.read.format(\"iceberg\").load(f\"{table_name}\")\n",
    "    to_delete = df.take(num_records)\n",
    "    for row in to_delete:\n",
    "        df = df.filter(~F.col(\"id_iceberg\").isin(row[\"id_iceberg\"]))\n",
    "    df.write.format(\"iceberg\").mode(\"overwrite\").save(f\"{table_name}\")\n",
    "\n",
    "\n",
    "# 주기적인 트랜잭션 수행\n",
    "def schedule_transactions(\n",
    "    table_name, transactions_per_second, total_duration_seconds, insert_ratio, update_ratio, delete_ratio\n",
    "):\n",
    "    schema = get_table_schema(table_name)\n",
    "    for _ in range(total_duration_seconds):\n",
    "        transactions: np.ndarray = np.random.choice(\n",
    "            [\"insert\", \"update\", \"delete\"],\n",
    "            size=transactions_per_second,\n",
    "            p=[insert_ratio, update_ratio, delete_ratio],\n",
    "        )\n",
    "        for _, action in enumerate(transactions):\n",
    "            if action == \"insert\":\n",
    "                insert_data(table_name, schema, num_records=1)\n",
    "            elif action == \"update\":\n",
    "                update_data(table_name, schema, num_records=1)\n",
    "            elif action == \"delete\":\n",
    "                delete_data(table_name, num_records=1)\n",
    "        print(f\"{[t[0] for t in transactions]}\")\n",
    "\n",
    "\n",
    "# 트랜잭션 실행\n",
    "for table in TABLE_LIST:\n",
    "    schedule_transactions(\n",
    "        table, TRANSACTIONS_PER_SECOND, TOTAL_DURATION_SECONDS, INSERT_RATIO, UPDATE_RATIO, DELETE_RATIO\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1964e49e009b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
